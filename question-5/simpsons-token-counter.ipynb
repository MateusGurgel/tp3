{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalando dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (1.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: google-generativeai in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (0.8.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-generativeai) (2.36.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-generativeai) (2.10.1)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-generativeai) (2.23.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-generativeai) (5.28.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-generativeai) (0.6.10)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-generativeai) (2.154.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-api-core->google-generativeai) (1.66.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from pydantic->google-generativeai) (2.27.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-api-core->google-generativeai) (1.68.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from google-api-core->google-generativeai) (1.68.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-decouple in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (3.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\mateus\\projects\\infnet\\promteng\\tp3\\venv\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install joblib\n",
    "%pip install tiktoken\n",
    "%pip install matplotlib\n",
    "%pip install -U google-generativeai\n",
    "%pip install python-decouple\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando a base de dados dos simpsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_script = pd.read_csv('./data/thesimpsons/simpsons_script_lines.csv', low_memory=False)\n",
    "df_episodes = pd.read_csv('./data/thesimpsons/simpsons_episodes.csv', low_memory=False)\n",
    "df_characters = pd.read_csv('./data/thesimpsons/simpsons_characters.csv', low_memory=False)\n",
    "df_locations = pd.read_csv('./data/thesimpsons/simpsons_locations.csv', low_memory=False)\n",
    "\n",
    "df_script.set_index('id', inplace=True)\n",
    "df_characters['id'] = df_characters['id'].astype(str)\n",
    "\n",
    "df_characters = df_characters.add_prefix('character_')\n",
    "df_locations = df_locations.add_prefix('location_')\n",
    "df_episodes = df_episodes.add_prefix('episode_')\n",
    "\n",
    "data = (\n",
    "    df_script.merge(df_episodes, left_on='episode_id', right_on='episode_id')\n",
    "             .merge(df_characters, left_on='character_id', right_on='character_id', how='left')\n",
    "             .merge(df_locations, left_on='location_id', right_on='location_id', how='left')\n",
    ")\n",
    "\n",
    "\n",
    "assert data.shape[0] == df_script.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "\n",
    "# Carga dos resumos\n",
    "summaries = joblib.load('../data/results/simpsons_episode_summary.joblib')\n",
    "print(summaries.keys())\n",
    "\n",
    "data = pd.read_parquet('../data/results/database_thesimpsons.parquet')\n",
    "\n",
    "\n",
    "data['line'] = (\"Espisode \" + data['episode_id'].astype(str) + ' | ' + \n",
    "                data['location_normalized_name'].fillna('') + ', ' + \n",
    "                data['character_normalized_name'].fillna('') + ' said: ' + \n",
    "                data['normalized_text'].fillna('')\n",
    ")\n",
    "\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimativa de Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def get_tokens(text: str):\n",
    "    encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoder.encode(text)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132087, 31)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.dropna(subset='normalized_text').copy()\n",
    "X['n_tokens'] = X.normalized_text.fillna('').apply(lambda x: len(get_tokens(x)))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculando a média de tokens por episódio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O episódio com mais tokens é o 49 com 3454 tokens.\n",
      "A média de tokens por episódio é de 2606.98\n"
     ]
    }
   ],
   "source": [
    "token_per_episode = X.groupby('episode_id').n_tokens.sum()\n",
    "general_token_mean = token_per_episode.mean()\n",
    "most_tokens_episode = token_per_episode.idxmax()\n",
    "\n",
    "print(\"O episódio com mais tokens é o\", most_tokens_episode, \"com\", token_per_episode[most_tokens_episode], \"tokens.\")\n",
    "print(\"A média de tokens por episódio é de {:.2f}\".format(general_token_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculando a média de tokens por temporada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A season com mais tokens é a 7 com 70418 tokens.\n",
      "A média de tokens por temporada é de 56551.46153846154\n"
     ]
    }
   ],
   "source": [
    "token_per_season = X.groupby('episode_season').n_tokens.sum()\n",
    "general_token_mean = token_per_season.mean()\n",
    "most_tokens_season = token_per_season.idxmax()\n",
    "\n",
    "print(\"A season com mais tokens é a\", most_tokens_season, \"com\", token_per_season[most_tokens_season], \"tokens.\")\n",
    "print(\"A média de tokens por temporada é de\", general_token_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Chaining para fazer uma análise descritiva das avaliações do IMDB e da audiência dos episódios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['episode_id', 'episode_season', 'episode_imdb_rating', 'episode_views']\n",
    "episode_stats = data[cols].drop_duplicates()\n",
    "episode_stats.to_csv('episode_stats.csv', sep=';', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levantando dados importantes sobre o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from decouple import config\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=config(\"GEMINI_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "csv_input = pd.read_csv('episode_stats.csv', nrows=100)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a data scientist specialized in creating descriptive analisys. \n",
    "['episode_id', 'episode_season', 'episode_imdb_rating', 'episode_views'] in series_data.csv.\n",
    "\n",
    "- series_data.csv: is a CSV file splitted by ';' and with the following columns:\n",
    "\n",
    "- episode_id: unique episode index\n",
    "- episode_season: episode season number\n",
    "- episode_imdb_rating: IMDB rating of the episode \n",
    "- episode_views: total views of the episode in milions.\n",
    "\n",
    "- All the data is from THE SIMPSONS series.\n",
    "\n",
    "1. The goal is to try to data, trends and a meaningful numbers of the data. \n",
    "3. make a mean of views and rating per season.\n",
    "4. Bring relevant data.\n",
    "5. do not add any thoughts or opinions.\n",
    "6. Talk only about data\n",
    "7. Do not Talk abou code or programming. Just data.\n",
    "8. Give the max quanity of data possible.\n",
    "\n",
    "\n",
    "<series_data.csv>\n",
    "    {csv_input}\n",
    "</series_data.csv>\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "important_data = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divagando sobre os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a data scientist specialized in creating insights about data\n",
    "\n",
    "You will recive a bunch of numbers, insights and trends about a dataset.\n",
    "\n",
    "1. The goal is to try to capture insights, trends and a meaningful analysis of the data. \n",
    "2. Just talk about the data, don't worry about the code.\n",
    "3. Talk A Lot of the data\n",
    "4. Make all the analysis you can.\n",
    "5. add thoughts and opinions.\n",
    "6. Allways be very detailed and specific.\n",
    "\n",
    "\n",
    "<data>\n",
    "    {important_data}\n",
    "</data>\n",
    "\"\"\"\n",
    "\n",
    "important_insights = model.generate_content(prompt).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilando em uma resposta final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a data scientist specialized in Teaching and resuming important Data\n",
    "\n",
    "You will recive a bunch of insights and information about a dataset.\n",
    "\n",
    "1. The goal is resume the data in a very detailed way.\n",
    "2. Just talk about the data, don't worry about the code.\n",
    "3. Resume the data in a very detailed way.\n",
    "4. Make everything very clear.\n",
    "5. add thoughts and opinions.\n",
    "6. Make a very detailed, but small resume of the data\n",
    "7. Be concise and clear. but do not miss any important information.\n",
    "\n",
    "<data>\n",
    "    {important_data}\n",
    "</data>\n",
    "\"\"\"\n",
    "\n",
    "important_insights = model.generate_content(prompt).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This dataset presents a descriptive analysis of IMDB ratings and viewership (in millions) for a sample of 100 Simpsons episodes drawn from seasons 2, 6, and 12.  The data is incomplete, representing only a fraction of the entire Simpsons series.  Therefore, generalizations about the entire show's performance should be avoided.\\n\\n**Key Findings:**\\n\\n* **Season-to-Season Variability:**  Significant differences exist in both average IMDB ratings and viewership across the three sampled seasons. Season 6 boasts the highest average IMDB rating (8.4) and viewership (69.96 million), while Season 12 shows the lowest average IMDB rating (7.15) and relatively lower viewership (49.44 million). Season 2 falls between the two.  This suggests a potential decline in both critical reception and viewership as the show progressed (at least within the sampled seasons).  This trend, however, might not hold for the whole series.\\n\\n* **Within-Season Variability:** The dataset indicates considerable variation in both IMDB ratings and viewership *within* each season.  This suggests the popularity of individual episodes varies greatly, even within a single season. This highlights the need for a more granular analysis focusing on individual episode data rather than only seasonal averages.\\n\\n* **Data Limitations:**  The most significant limitation is the dataset's incompleteness.  Analyzing only three seasons (with an unspecified number of episodes per season) prevents drawing conclusions about the show's overall performance and its evolution across all 30+ seasons.  Furthermore,  the lack of individual episode-level data within each season limits the depth of analysis possible.  More data is required for a more robust study.\\n\\n**Suggestions for Future Analysis:**\\n\\nA more comprehensive analysis should include:\\n\\n1. **Complete Data:**  Data for all seasons and all episodes is crucial.\\n2. **Individual Episode Data:** Detailed data points for each episode, including ratings and views, would allow for more in-depth analysis.\\n3. **Correlation Analysis:**  Exploring the correlation between IMDB ratings and viewership could reveal interesting relationships.\\n4. **Regression Analysis:**  Analyzing factors influencing viewership (e.g., time of broadcast, special events) could provide valuable insights.\\n5. **Time Series Analysis:**  Analyzing viewership trends over time could provide a clearer picture of the show’s popularity trajectory.\\n\\n\\nIn summary, this dataset provides a preliminary glimpse into the popularity of the Simpsons across a small subset of episodes. However, significant limitations necessitate a much larger and more complete dataset to draw meaningful conclusions.\\n\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
